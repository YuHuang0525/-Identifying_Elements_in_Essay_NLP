{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee2f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c00f6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f167bc7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144288</th>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618153e+12</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>3203.0</td>\n",
       "      <td>if I'm not sure what college I want to attend...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>386 387 388 389 390 391 392 393 394 395 396 39...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144289</th>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618153e+12</td>\n",
       "      <td>3221.0</td>\n",
       "      <td>4509.0</td>\n",
       "      <td>seeking multiple opinions before making a har...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 3</td>\n",
       "      <td>576 577 578 579 580 581 582 583 584 585 586 58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144290</th>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618025e+12</td>\n",
       "      <td>4510.0</td>\n",
       "      <td>4570.0</td>\n",
       "      <td>it is better to seek multiple opinions instead...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>828 829 830 831 832 833 834 835 836 837 838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144291</th>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618025e+12</td>\n",
       "      <td>4570.0</td>\n",
       "      <td>4922.0</td>\n",
       "      <td>The impact of asking people to help you make a...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 4</td>\n",
       "      <td>839 840 841 842 843 844 845 846 847 848 849 85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144292</th>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618025e+12</td>\n",
       "      <td>4935.0</td>\n",
       "      <td>5825.0</td>\n",
       "      <td>there are many other reasons one might want to...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Concluding Statement 1</td>\n",
       "      <td>905 906 907 908 909 910 911 912 913 914 915 91...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144293 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  discourse_id  discourse_start  discourse_end  \\\n",
       "0       423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1       423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2       423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3       423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4       423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "...              ...           ...              ...            ...   \n",
       "144288  4C471936CD75  1.618153e+12           2234.0         3203.0   \n",
       "144289  4C471936CD75  1.618153e+12           3221.0         4509.0   \n",
       "144290  4C471936CD75  1.618025e+12           4510.0         4570.0   \n",
       "144291  4C471936CD75  1.618025e+12           4570.0         4922.0   \n",
       "144292  4C471936CD75  1.618025e+12           4935.0         5825.0   \n",
       "\n",
       "                                           discourse_text  \\\n",
       "0       Modern humans today are always on their phone....   \n",
       "1       They are some really bad consequences when stu...   \n",
       "2       Some certain areas in the United States ban ph...   \n",
       "3       When people have phones, they know about certa...   \n",
       "4       Driving is one of the way how to get around. P...   \n",
       "...                                                   ...   \n",
       "144288   if I'm not sure what college I want to attend...   \n",
       "144289   seeking multiple opinions before making a har...   \n",
       "144290  it is better to seek multiple opinions instead...   \n",
       "144291  The impact of asking people to help you make a...   \n",
       "144292  there are many other reasons one might want to...   \n",
       "\n",
       "              discourse_type      discourse_type_num  \\\n",
       "0                       Lead                  Lead 1   \n",
       "1                   Position              Position 1   \n",
       "2                   Evidence              Evidence 1   \n",
       "3                   Evidence              Evidence 2   \n",
       "4                      Claim                 Claim 1   \n",
       "...                      ...                     ...   \n",
       "144288              Evidence              Evidence 2   \n",
       "144289              Evidence              Evidence 3   \n",
       "144290              Position              Position 1   \n",
       "144291              Evidence              Evidence 4   \n",
       "144292  Concluding Statement  Concluding Statement 1   \n",
       "\n",
       "                                         predictionstring  \n",
       "0       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1            45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3       76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4       139 140 141 142 143 144 145 146 147 148 149 15...  \n",
       "...                                                   ...  \n",
       "144288  386 387 388 389 390 391 392 393 394 395 396 39...  \n",
       "144289  576 577 578 579 580 581 582 583 584 585 586 58...  \n",
       "144290        828 829 830 831 832 833 834 835 836 837 838  \n",
       "144291  839 840 841 842 843 844 845 846 847 848 849 85...  \n",
       "144292  905 906 907 908 909 910 911 912 913 914 915 91...  \n",
       "\n",
       "[144293 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699588d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path('train')\n",
    "\n",
    "def get_raw_text(ids):\n",
    "    with open(path/f'{ids}.txt', 'r') as file: data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b4025",
   "metadata": {},
   "source": [
    "# Tokenize the text and assign labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5da0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_idx_label(x):\n",
    "    start = x['discourse_start']\n",
    "    end = x['discourse_end']\n",
    "    return (x['discourse_type'], start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4841d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['idx_label'] = dataset.apply(combine_idx_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5de6821f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>idx_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>[(Position, 0.0, 170.0), (Evidence, 170.0, 357...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[(Lead, 0.0, 455.0), (Position, 456.0, 592.0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[(Position, 17.0, 56.0), (Counterclaim, 64.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>[(Lead, 0.0, 160.0), (Evidence, 161.0, 872.0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[(Position, 0.0, 57.0), (Claim, 58.0, 91.0), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15589</th>\n",
       "      <td>FFF1442D6698</td>\n",
       "      <td>[(Lead, 0.0, 639.0), (Position, 640.0, 710.0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15590</th>\n",
       "      <td>FFF1ED4F8544</td>\n",
       "      <td>[(Lead, 0.0, 351.0), (Position, 351.0, 457.0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15591</th>\n",
       "      <td>FFF868E06176</td>\n",
       "      <td>[(Lead, 0.0, 225.0), (Position, 226.0, 324.0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15592</th>\n",
       "      <td>FFFD0AF13501</td>\n",
       "      <td>[(Claim, 237.0, 280.0), (Claim, 281.0, 347.0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15593</th>\n",
       "      <td>FFFF80B8CC2F</td>\n",
       "      <td>[(Evidence, 0.0, 990.0)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15594 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                          idx_label\n",
       "0      0000D23A521A  [(Position, 0.0, 170.0), (Evidence, 170.0, 357...\n",
       "1      00066EA9880D  [(Lead, 0.0, 455.0), (Position, 456.0, 592.0),...\n",
       "2      000E6DE9E817  [(Position, 17.0, 56.0), (Counterclaim, 64.0, ...\n",
       "3      001552828BD0  [(Lead, 0.0, 160.0), (Evidence, 161.0, 872.0),...\n",
       "4      0016926B079C  [(Position, 0.0, 57.0), (Claim, 58.0, 91.0), (...\n",
       "...             ...                                                ...\n",
       "15589  FFF1442D6698  [(Lead, 0.0, 639.0), (Position, 640.0, 710.0),...\n",
       "15590  FFF1ED4F8544  [(Lead, 0.0, 351.0), (Position, 351.0, 457.0),...\n",
       "15591  FFF868E06176  [(Lead, 0.0, 225.0), (Position, 226.0, 324.0),...\n",
       "15592  FFFD0AF13501  [(Claim, 237.0, 280.0), (Claim, 281.0, 347.0),...\n",
       "15593  FFFF80B8CC2F                           [(Evidence, 0.0, 990.0)]\n",
       "\n",
       "[15594 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_pd = pd.DataFrame(dataset.groupby('id')['idx_label'].apply(list))\n",
    "temp_pd = temp_pd.reset_index()\n",
    "temp_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c250214",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['raw_text'] = dataset['id'].apply(get_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be114d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A8445CABFECE</td>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6B4F7A0165B9</td>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>People are debating whether if drivers should ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>Texting and driving\\n\\nOver half of drivers in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144240</th>\n",
       "      <td>8F4B595CF9E7</td>\n",
       "      <td>Do you ever want more opinions and options whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144252</th>\n",
       "      <td>6B5809C83978</td>\n",
       "      <td>Has anyone ever gave you advice? Was the advic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144261</th>\n",
       "      <td>408A7D3D2EEC</td>\n",
       "      <td>Imagine seeking advice from multiple people an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144270</th>\n",
       "      <td>AFEC37C2D43F</td>\n",
       "      <td>There has been at least one point in everyone'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144284</th>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>In ancient times, and also still today in some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15594 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                           raw_text\n",
       "0       423A1CA112E2  Phones\\n\\nModern humans today are always on th...\n",
       "10      A8445CABFECE  Phones & Driving\\n\\nDrivers should not be able...\n",
       "14      6B4F7A0165B9  Cell Phone Operation While Driving\\n\\nThe abil...\n",
       "21      E05C7F5C1156  People are debating whether if drivers should ...\n",
       "33      50B3435E475B  Texting and driving\\n\\nOver half of drivers in...\n",
       "...              ...                                                ...\n",
       "144240  8F4B595CF9E7  Do you ever want more opinions and options whe...\n",
       "144252  6B5809C83978  Has anyone ever gave you advice? Was the advic...\n",
       "144261  408A7D3D2EEC  Imagine seeking advice from multiple people an...\n",
       "144270  AFEC37C2D43F  There has been at least one point in everyone'...\n",
       "144284  4C471936CD75  In ancient times, and also still today in some...\n",
       "\n",
       "[15594 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[['id', 'raw_text']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adc5e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_dataset = temp_pd.merge(dataset[['id', 'raw_text']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09670c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>idx_label</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>[(Position, 0.0, 170.0), (Evidence, 170.0, 357...</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[(Lead, 0.0, 455.0), (Position, 456.0, 592.0),...</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[(Position, 17.0, 56.0), (Counterclaim, 64.0, ...</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>[(Lead, 0.0, 160.0), (Evidence, 161.0, 872.0),...</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[(Position, 0.0, 57.0), (Claim, 58.0, 91.0), (...</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15589</th>\n",
       "      <td>FFF1442D6698</td>\n",
       "      <td>[(Lead, 0.0, 639.0), (Position, 640.0, 710.0),...</td>\n",
       "      <td>Every student looks forward to summer break, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15590</th>\n",
       "      <td>FFF1ED4F8544</td>\n",
       "      <td>[(Lead, 0.0, 351.0), (Position, 351.0, 457.0),...</td>\n",
       "      <td>Many citizens argue that the Electoral college...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15591</th>\n",
       "      <td>FFF868E06176</td>\n",
       "      <td>[(Lead, 0.0, 225.0), (Position, 226.0, 324.0),...</td>\n",
       "      <td>Every summer break, students are given project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15592</th>\n",
       "      <td>FFFD0AF13501</td>\n",
       "      <td>[(Claim, 237.0, 280.0), (Claim, 281.0, 347.0),...</td>\n",
       "      <td>In the article \"A Cowboy Who Rode the Waves\" L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15593</th>\n",
       "      <td>FFFF80B8CC2F</td>\n",
       "      <td>[(Evidence, 0.0, 990.0)]</td>\n",
       "      <td>Venus is a planet what belong the System Solar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15594 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                          idx_label  \\\n",
       "0      0000D23A521A  [(Position, 0.0, 170.0), (Evidence, 170.0, 357...   \n",
       "1      00066EA9880D  [(Lead, 0.0, 455.0), (Position, 456.0, 592.0),...   \n",
       "2      000E6DE9E817  [(Position, 17.0, 56.0), (Counterclaim, 64.0, ...   \n",
       "3      001552828BD0  [(Lead, 0.0, 160.0), (Evidence, 161.0, 872.0),...   \n",
       "4      0016926B079C  [(Position, 0.0, 57.0), (Claim, 58.0, 91.0), (...   \n",
       "...             ...                                                ...   \n",
       "15589  FFF1442D6698  [(Lead, 0.0, 639.0), (Position, 640.0, 710.0),...   \n",
       "15590  FFF1ED4F8544  [(Lead, 0.0, 351.0), (Position, 351.0, 457.0),...   \n",
       "15591  FFF868E06176  [(Lead, 0.0, 225.0), (Position, 226.0, 324.0),...   \n",
       "15592  FFFD0AF13501  [(Claim, 237.0, 280.0), (Claim, 281.0, 347.0),...   \n",
       "15593  FFFF80B8CC2F                           [(Evidence, 0.0, 990.0)]   \n",
       "\n",
       "                                                raw_text  \n",
       "0      Some people belive that the so called \"face\" o...  \n",
       "1      Driverless cars are exaclty what you would exp...  \n",
       "2      Dear: Principal\\n\\nI am arguing against the po...  \n",
       "3      Would you be able to give your car up? Having ...  \n",
       "4      I think that students would benefit from learn...  \n",
       "...                                                  ...  \n",
       "15589  Every student looks forward to summer break, i...  \n",
       "15590  Many citizens argue that the Electoral college...  \n",
       "15591  Every summer break, students are given project...  \n",
       "15592  In the article \"A Cowboy Who Rode the Waves\" L...  \n",
       "15593  Venus is a planet what belong the System Solar...  \n",
       "\n",
       "[15594 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revised_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75c00178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revised_dataset['idx_label'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f117191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75b470f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "(\"Expected bytes, got a 'float' object\", 'Conversion failed for column idx_label with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m0/r4gc8d8x2dz_h2ymv3jgxbv80000gn/T/ipykernel_39840/1518441386.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrevised_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[0;34m(cls, df, features, info, split)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInMemoryTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/datasets/table.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minject_arrow_table_documentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_fut\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_fut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_fut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    579\u001b[0m             e.args += (\"Conversion failed for column {!s} with type {!s}\"\n\u001b[1;32m    580\u001b[0m                        .format(col.name, col.dtype),)\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfield_nullable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             raise ValueError(\"Field {} was non-nullable but pandas column \"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         except (pa.ArrowInvalid,\n\u001b[1;32m    577\u001b[0m                 \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: (\"Expected bytes, got a 'float' object\", 'Conversion failed for column idx_label with type object')"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(revised_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335ddf0",
   "metadata": {},
   "source": [
    "# 上面这个说明dataset的format不能是list of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e734ac3",
   "metadata": {},
   "source": [
    "修改dataset dataframe的format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5b2dec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>[0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...</td>\n",
       "      <td>[170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...</td>\n",
       "      <td>[Position, Evidence, Evidence, Claim, Counterc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...</td>\n",
       "      <td>[455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...</td>\n",
       "      <td>[56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>[0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...</td>\n",
       "      <td>[160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...</td>\n",
       "      <td>[Lead, Evidence, Claim, Claim, Evidence, Claim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...</td>\n",
       "      <td>[57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15589</th>\n",
       "      <td>FFF1442D6698</td>\n",
       "      <td>[0.0, 640.0, 710.0, 778.0, 841.0, 1291.0, 2104...</td>\n",
       "      <td>[639.0, 710.0, 778.0, 826.0, 1222.0, 2103.0, 2...</td>\n",
       "      <td>[Lead, Position, Claim, Claim, Evidence, Evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15590</th>\n",
       "      <td>FFF1ED4F8544</td>\n",
       "      <td>[0.0, 351.0, 458.0, 513.0, 555.0, 606.0, 767.0...</td>\n",
       "      <td>[351.0, 457.0, 512.0, 555.0, 605.0, 767.0, 183...</td>\n",
       "      <td>[Lead, Position, Claim, Claim, Claim, Claim, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15591</th>\n",
       "      <td>FFF868E06176</td>\n",
       "      <td>[0.0, 226.0, 417.0, 502.0, 996.0, 1089.0, 1364...</td>\n",
       "      <td>[225.0, 324.0, 501.0, 914.0, 1088.0, 1363.0, 1...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15592</th>\n",
       "      <td>FFFD0AF13501</td>\n",
       "      <td>[237.0, 281.0, 348.0, 431.0, 517.0, 584.0, 959...</td>\n",
       "      <td>[280.0, 347.0, 431.0, 516.0, 583.0, 943.0, 105...</td>\n",
       "      <td>[Claim, Claim, Evidence, Claim, Claim, Evidenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15593</th>\n",
       "      <td>FFFF80B8CC2F</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[990.0]</td>\n",
       "      <td>[Evidence]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15594 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                    discourse_start  \\\n",
       "0      0000D23A521A  [0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...   \n",
       "1      00066EA9880D  [0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...   \n",
       "2      000E6DE9E817  [17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...   \n",
       "3      001552828BD0  [0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...   \n",
       "4      0016926B079C  [0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...   \n",
       "...             ...                                                ...   \n",
       "15589  FFF1442D6698  [0.0, 640.0, 710.0, 778.0, 841.0, 1291.0, 2104...   \n",
       "15590  FFF1ED4F8544  [0.0, 351.0, 458.0, 513.0, 555.0, 606.0, 767.0...   \n",
       "15591  FFF868E06176  [0.0, 226.0, 417.0, 502.0, 996.0, 1089.0, 1364...   \n",
       "15592  FFFD0AF13501  [237.0, 281.0, 348.0, 431.0, 517.0, 584.0, 959...   \n",
       "15593  FFFF80B8CC2F                                              [0.0]   \n",
       "\n",
       "                                           discourse_end  \\\n",
       "0      [170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...   \n",
       "1      [455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...   \n",
       "2      [56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....   \n",
       "3      [160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...   \n",
       "4      [57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...   \n",
       "...                                                  ...   \n",
       "15589  [639.0, 710.0, 778.0, 826.0, 1222.0, 2103.0, 2...   \n",
       "15590  [351.0, 457.0, 512.0, 555.0, 605.0, 767.0, 183...   \n",
       "15591  [225.0, 324.0, 501.0, 914.0, 1088.0, 1363.0, 1...   \n",
       "15592  [280.0, 347.0, 431.0, 516.0, 583.0, 943.0, 105...   \n",
       "15593                                            [990.0]   \n",
       "\n",
       "                                          discourse_type  \n",
       "0      [Position, Evidence, Evidence, Claim, Counterc...  \n",
       "1      [Lead, Position, Claim, Evidence, Claim, Evide...  \n",
       "2      [Position, Counterclaim, Rebuttal, Evidence, C...  \n",
       "3      [Lead, Evidence, Claim, Claim, Evidence, Claim...  \n",
       "4      [Position, Claim, Claim, Claim, Claim, Evidenc...  \n",
       "...                                                  ...  \n",
       "15589  [Lead, Position, Claim, Claim, Evidence, Evide...  \n",
       "15590  [Lead, Position, Claim, Claim, Claim, Claim, E...  \n",
       "15591  [Lead, Position, Claim, Evidence, Claim, Evide...  \n",
       "15592  [Claim, Claim, Evidence, Claim, Claim, Evidenc...  \n",
       "15593                                         [Evidence]  \n",
       "\n",
       "[15594 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = pd.DataFrame(dataset.groupby('id')['discourse_start'].apply(list))\n",
    "temp1 = temp1.reset_index()\n",
    "temp2 = pd.DataFrame(dataset.groupby('id')['discourse_end'].apply(list))\n",
    "temp2 = temp2.reset_index()\n",
    "temp3 = pd.DataFrame(dataset.groupby('id')['discourse_type'].apply(list))\n",
    "temp3 = temp3.reset_index()\n",
    "temp1 = temp1.merge(temp2)\n",
    "temp1 = temp1.merge(temp3)\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f56c825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1['raw_text'] = temp1['id'].apply(get_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd7e7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba21d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ds.train_test_split(test_size=0.1, shuffle=True, seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "512c0186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'raw_text', '__index_level_0__'],\n",
       "        num_rows: 14034\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'raw_text', '__index_level_0__'],\n",
       "        num_rows: 1560\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e334f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', \n",
    "                                          add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42a99b58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '88BEB108EA83',\n",
       " 'discourse_start': [21.0, 120.0, 213.0, 429.0, 590.0, 760.0],\n",
       " 'discourse_end': [111.0, 212.0, 428.0, 589.0, 759.0, 850.0],\n",
       " 'discourse_type': ['Position',\n",
       "  'Claim',\n",
       "  'Evidence',\n",
       "  'Evidence',\n",
       "  'Claim',\n",
       "  'Concluding Statement'],\n",
       " 'raw_text': \"Dear State Senator,\\n\\nI think that we should get rid of the eletoral collage and switch to a popular vote system because with the popular vote we can get the president we want if more people vote for the same one. Its fair if you vote for one preisdent but lose the election because more people voted for the other president, but its not fair if more people vote for the same president and still lose due to the eletoral collage\\n\\nNow the eletoral collage is good for keeping track of votes but now most of the voting nowadays most of the voting is done off an eletronic device of some sort. Its not fair that almost all of the states' eletoral collages are winner-take-all. so its like turning the votes around and voting for the other person when you didn't.\\n\\nThank you for taking the time to read this and I hope to see the rules changing real soon.\\n\\nSincearly, PROPER_NAME.    \",\n",
       " '__index_level_0__': 8295}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "54f99efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'o': 0,\n",
    "             'B-Lead': 1,\n",
    "             'I-Lead': 2,\n",
    "             'B-Position': 3,\n",
    "             'I-Position': 4,\n",
    "             'B-Evidence': 5,\n",
    "             'I-Evidence': 6,\n",
    "             'B-Claim': 7,\n",
    "             'I-Claim': 8,\n",
    "             'B-Concluding Statement': 9,\n",
    "             'I-Concluding Statement': 10,\n",
    "             'B-Counterclaim': 11,\n",
    "             'I-Counterclaim': 12, \n",
    "             'B-Rebuttal': 13,\n",
    "             'I-Rebuttal':14,\n",
    "             'special':15}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2412d",
   "metadata": {},
   "source": [
    "# 下面这个function 是整个文档最为关键的function\n",
    "# 也是 hy智慧的结晶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "384ecd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_assign_labels(x):\n",
    "    try:\n",
    "        o = tokenizer(x['raw_text'], truncation=True, padding=True,\n",
    "                      max_length=1024, return_offsets_mapping=True)\n",
    "\n",
    "        offset = o['offset_mapping']\n",
    "\n",
    "        labels = [0] * len(offset)\n",
    "        i = 0\n",
    "        while i < len(offset):\n",
    "            if offset[i][0] == offset[i][1]:\n",
    "                    labels[i] = label_dict['special']\n",
    "                    i += 1\n",
    "                    continue\n",
    "            elif offset[i][0] in x['discourse_start']:\n",
    "                temp_idx = x['discourse_start'].index(offset[i][0])\n",
    "                temp_label = 'B-' + x['discourse_type'][temp_idx]\n",
    "                temp_idx_end = x['discourse_end'][temp_idx]\n",
    "                labels[i] = label_dict[temp_label]\n",
    "\n",
    "                temp_label = 'I-' + x['discourse_type'][temp_idx]\n",
    "                j = i+1\n",
    "                while j != len(offset):\n",
    "                    if offset[j][0] < temp_idx_end:\n",
    "                        labels[j] = label_dict[temp_label]\n",
    "                        j += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                i = j\n",
    "                continue\n",
    "            else:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        o['labels'] = labels\n",
    "        return o        \n",
    "    except:\n",
    "        print(x['id'])\n",
    "        print(j)\n",
    "        print(temp_idx_end)\n",
    "        print(temp_label)\n",
    "        print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "051ed018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokenize_assign_labels(x):\n",
    "    o = tokenizer(x['raw_text'], truncation=True, padding=True,\n",
    "                  max_length=1024, return_offsets_mapping=True)\n",
    "\n",
    "    offset = o['offset_mapping']\n",
    "\n",
    "    labels = [0] * len(offset)\n",
    "    i = 0\n",
    "    while i < len(offset):\n",
    "        if offset[i][0] == offset[i][1]:\n",
    "                labels[i] = label_dict['special']\n",
    "                i += 1\n",
    "                continue\n",
    "        elif offset[i][0] in x['discourse_start']:\n",
    "            temp_idx = x['discourse_start'].index(offset[i][0])\n",
    "            temp_label = 'B-' + x['discourse_type'][temp_idx]\n",
    "            temp_idx_end = x['discourse_end'][temp_idx]\n",
    "            labels[i] = label_dict[temp_label]\n",
    "\n",
    "            temp_label = 'I-' + x['discourse_type'][temp_idx]\n",
    "            j = i+1\n",
    "            while j != len(offset):\n",
    "                if offset[j][0] < temp_idx_end:\n",
    "                    labels[j] = label_dict[temp_label]\n",
    "                    j += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            i = j\n",
    "            continue\n",
    "        else:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "    o['labels'] = labels\n",
    "    return o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0158ec14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 497, 78, 802, 6, 4472, 2239, 189, 2045, 101, 41, 1365, 169, 66, 13, 1159, 54, 95, 236, 7, 14514, 334, 7, 45, 28, 547, 8943, 13, 24, 131, 959, 6, 150, 24, 189, 2045, 3231, 12, 38459, 7, 1157, 521, 7, 2845, 7, 185, 49, 4050, 804, 6, 24, 64, 747, 498, 28, 182, 10142, 13, 258, 167, 54, 185, 2093, 9, 5, 1973, 8, 167, 54, 535, 10, 2065, 2718, 9, 334, 4, 870, 2455, 521, 5, 1973, 7, 2807, 49, 2239, 1737, 172, 24, 8382, 521, 2845, 549, 51, 1532, 275, 31, 5, 5863, 9, 49, 308, 184, 6, 10, 8171, 6, 50, 4558, 1493, 4, 96, 1285, 6, 1311, 521, 5, 1973, 7, 2845, 147, 8, 141, 51, 1532, 275, 2386, 55, 4736, 521, 7, 1095, 184, 1195, 87, 283, 7, 334, 142, 51, 351, 75, 33, 7, 4022, 59, 3064, 639, 15, 334, 6014, 4, 993, 82, 189, 206, 4472, 2239, 16, 95, 10, 169, 13, 1159, 7, 185, 41, 1365, 169, 66, 9, 49, 1265, 6, 1622, 142, 51, 32, 22414, 131, 77, 6, 11, 754, 6, 4472, 2239, 16, 45, 129, 10, 169, 13, 408, 7, 676, 430, 1380, 11019, 6, 53, 24, 16, 67, 10, 169, 13, 521, 7, 109, 49, 233, 11, 4881, 5, 2504, 9, 16759, 26092, 6357, 4, 50118, 50118, 243, 16, 10, 1537, 6563, 14, 5, 275, 1265, 606, 31, 1025, 10, 8171, 8, 97, 839, 9, 1265, 32, 7421, 13514, 87, 10, 8171, 1265, 4, 152, 189, 28, 1528, 13, 171, 82, 53, 24, 16, 2299, 45, 1528, 13, 961, 4, 4210, 16943, 32, 747, 7337, 8, 11138, 61, 2386, 190, 5, 144, 157, 6589, 196, 521, 5, 778, 7, 28, 16573, 30, 49, 6763, 4, 7411, 209, 15083, 32, 818, 37580, 868, 61, 3315, 5, 2948, 7, 912, 190, 667, 7, 6396, 4, 152, 6, 11, 1004, 6, 3607, 521, 54, 3127, 770, 7, 1532, 23, 10, 22025, 142, 122, 49, 3254, 34, 685, 49, 10563, 7, 6396, 4, 40438, 2239, 64, 45, 129, 1888, 5, 2166, 1380, 10070, 8, 146, 106, 55, 25832, 6, 53, 24, 64, 67, 1157, 521, 54, 33, 10, 16699, 7, 28, 55, 3236, 7180, 5, 1973, 7, 192, 114, 8201, 1235, 31, 5, 2621, 9, 82, 2607, 106, 1056, 357, 15, 49, 173, 4, 14747, 31, 5, 5984, 1795, 6, 89, 64, 67, 28, 474, 1795, 4, 50118, 50118, 3762, 9, 5, 934, 2188, 5467, 64, 2504, 98, 1335, 11, 334, 16, 142, 521, 619, 14, 3867, 51, 32, 98, 4736, 14, 51, 1395, 5043, 11, 334, 51, 197, 283, 7, 334, 142, 49, 6856, 25473, 29, 5, 801, 2504, 9, 41, 5467, 4, 616, 51, 189, 28, 441, 7, 146, 24, 149, 70, 9, 49, 4050, 396, 203, 696, 6, 5, 6976, 122, 5712, 2500, 70, 49, 18295, 54, 58, 11, 1511, 19, 106, 70, 183, 4, 870, 2455, 521, 7, 109, 4472, 2239, 6, 42, 696, 9, 6856, 16, 9820, 142, 5, 521, 54, 32, 4736, 53, 45, 3606, 350, 4008, 31, 5, 5298, 40, 28, 1220, 7, 2725, 49, 4050, 31, 184, 98, 51, 64, 185, 5, 183, 160, 7, 244, 2097, 5, 2504, 9, 5467, 396, 519, 7, 4022, 59, 70, 5, 173, 51, 189, 33, 2039, 4, 96, 1285, 6, 42, 64, 244, 10, 3143, 9, 521, 54, 2649, 334, 13, 3112, 5788, 9, 86, 142, 9, 743, 215, 25, 1356, 6, 1098, 1938, 6, 8, 190, 103, 2536, 474, 743, 4, 318, 209, 521, 216, 51, 64, 2773, 146, 62, 5, 173, 137, 51, 190, 671, 7, 334, 24, 64, 185, 65, 540, 3992, 160, 106, 98, 51, 64, 1056, 15, 3046, 696, 51, 32, 4098, 19, 4, 152, 189, 888, 898, 11, 10, 7280, 9, 5, 5933, 9, 86, 14, 521, 32, 66, 9, 334, 4, 50118, 50118, 10787, 54, 9449, 2455, 4472, 2239, 189, 224, 14, 521, 54, 109, 45, 2725, 1380, 40, 33, 55, 26434, 6, 8, 3891, 10, 3007, 1265, 131, 959, 6, 209, 82, 26506, 7856, 19593, 6856, 19, 10, 205, 1265, 4, 17482, 498, 521, 11, 1380, 54, 32, 16573, 40, 253, 62, 602, 97, 521, 108, 1056, 552, 19, 106, 25, 49, 17447, 21943, 29, 643, 4, 6983, 608, 4472, 2239, 51, 351, 75, 33, 7, 4022, 59, 751, 26434, 98, 521, 54, 3127, 236, 7, 1056, 64, 342, 1235, 11, 41, 1737, 14, 275, 10401, 49, 2239, 2496, 4, 2044, 477, 82, 54, 9449, 17679, 4472, 2239, 189, 146, 16, 14, 30, 2455, 521, 5, 2031, 7, 1532, 31, 184, 171, 40, 95, 912, 49, 3218, 70, 561, 4, 616, 11, 103, 1200, 42, 189, 28, 1528, 6, 5, 82, 54, 74, 912, 49, 1265, 77, 576, 5, 1973, 9, 4472, 2239, 32, 5, 276, 82, 54, 74, 1874, 66, 9, 239, 334, 190, 396, 4472, 2239, 4, 870, 1311, 521, 55, 797, 81, 49, 1265, 149, 4472, 2239, 5, 82, 54, 236, 7, 6726, 32, 576, 55, 1915, 7, 617, 49, 1265, 61, 16, 3127, 505, 4, 50118, 50118, 39173, 521, 5, 1973, 7, 33, 4472, 2239, 115, 28, 2778, 10142, 7, 45, 95, 5, 2172, 54, 2845, 7, 860, 24, 6, 53, 24, 64, 67, 28, 10142, 13, 167, 54, 619, 51, 240, 5, 11, 621, 676, 19, 10, 3254, 4, 85, 16, 41, 19717, 15480, 14, 77, 314, 7, 49, 308, 2110, 521, 40, 95, 6602, 2239, 70, 561, 6, 53, 42, 16, 45, 5705, 5, 403, 4, 1590, 171, 7576, 147, 334, 1743, 32, 145, 373, 88, 864, 50, 1986, 32, 145, 355, 5, 1756, 817, 5, 4795, 14, 5, 464, 40, 1303, 41, 1374, 7280, 9, 8106, 1195, 87, 41, 712, 4, 3791, 42, 189, 28, 142, 82, 32, 16292, 7, 9375, 464, 98, 95, 101, 1311, 521, 1736, 7796, 34, 1006, 11, 171, 334, 1743, 6, 5, 1973, 9, 4472, 2239, 189, 65, 183, 3364, 7, 1477, 55, 87, 24, 8357, 4, 635, 6, 5, 129, 169, 7, 3127, 192, 114, 4472, 2239, 1364, 50, 45, 16, 7, 888, 5731, 24, 11, 1304, 6, 190, 13, 95, 10, 76, 6, 7, 7118, 5, 414, 8, 2845, 549, 50, 45, 24, 197, 28, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 8), (9, 16), (16, 17), (18, 26), (27, 35), (36, 39), (40, 44), (45, 49), (50, 52), (53, 57), (58, 61), (62, 65), (66, 69), (70, 74), (75, 78), (79, 83), (84, 88), (89, 91), (92, 96), (97, 103), (104, 106), (107, 110), (111, 113), (114, 118), (119, 130), (131, 134), (135, 137), (137, 138), (139, 146), (146, 147), (148, 153), (154, 156), (157, 160), (161, 165), (166, 173), (173, 174), (174, 183), (184, 186), (187, 192), (193, 201), (202, 204), (205, 211), (212, 214), (215, 219), (220, 225), (226, 233), (234, 240), (240, 241), (242, 244), (245, 248), (249, 254), (255, 260), (261, 263), (264, 268), (269, 279), (280, 283), (284, 288), (289, 294), (295, 298), (299, 303), (304, 313), (314, 316), (317, 320), (321, 327), (328, 331), (332, 337), (338, 341), (342, 350), (351, 352), (353, 364), (365, 369), (370, 372), (373, 379), (379, 380), (381, 383), (384, 392), (393, 401), (402, 405), (406, 412), (413, 415), (416, 422), (423, 428), (429, 437), (438, 449), (450, 454), (455, 457), (458, 462), (463, 471), (472, 478), (479, 486), (487, 491), (492, 497), (498, 502), (503, 507), (508, 511), (512, 519), (520, 522), (523, 528), (529, 532), (533, 537), (537, 538), (539, 540), (541, 550), (550, 551), (552, 554), (555, 563), (564, 568), (568, 569), (570, 572), (573, 581), (581, 582), (583, 589), (590, 598), (599, 602), (603, 609), (610, 612), (613, 619), (620, 625), (626, 629), (630, 633), (634, 638), (639, 644), (645, 649), (650, 656), (657, 661), (662, 666), (667, 675), (676, 678), (679, 683), (684, 688), (689, 695), (696, 700), (701, 705), (706, 708), (709, 715), (716, 723), (724, 728), (729, 732), (732, 734), (735, 739), (740, 742), (743, 748), (749, 754), (755, 762), (763, 769), (770, 772), (773, 779), (779, 783), (783, 784), (785, 789), (790, 796), (797, 800), (801, 806), (807, 815), (816, 824), (825, 827), (828, 832), (833, 834), (835, 838), (839, 842), (843, 847), (848, 850), (851, 855), (856, 858), (859, 863), (864, 867), (868, 871), (872, 874), (875, 880), (881, 890), (890, 891), (892, 898), (899, 906), (907, 911), (912, 915), (916, 920), (920, 921), (922, 926), (926, 927), (928, 930), (931, 935), (935, 936), (937, 945), (946, 954), (955, 957), (958, 961), (962, 966), (967, 968), (969, 972), (973, 976), (977, 985), (986, 988), (989, 999), (1000, 1009), (1010, 1015), (1016, 1022), (1022, 1023), (1024, 1027), (1028, 1030), (1031, 1033), (1034, 1038), (1039, 1040), (1041, 1044), (1045, 1048), (1049, 1057), (1058, 1060), (1061, 1063), (1064, 1069), (1070, 1074), (1075, 1077), (1078, 1086), (1087, 1090), (1091, 1097), (1098, 1100), (1101, 1107), (1107, 1113), (1114, 1122), (1122, 1123), (1123, 1124), (1124, 1125), (1125, 1127), (1128, 1130), (1131, 1132), (1133, 1139), (1140, 1146), (1147, 1151), (1152, 1155), (1156, 1160), (1161, 1170), (1171, 1176), (1177, 1181), (1182, 1188), (1189, 1190), (1191, 1200), (1201, 1204), (1205, 1210), (1211, 1216), (1217, 1219), (1220, 1229), (1230, 1233), (1234, 1241), (1242, 1248), (1249, 1253), (1254, 1255), (1256, 1265), (1266, 1275), (1275, 1276), (1277, 1281), (1282, 1285), (1286, 1288), (1289, 1293), (1294, 1297), (1298, 1302), (1303, 1309), (1310, 1313), (1314, 1316), (1317, 1319), (1320, 1330), (1331, 1334), (1335, 1339), (1340, 1343), (1344, 1352), (1352, 1353), (1354, 1359), (1359, 1364), (1365, 1368), (1369, 1374), (1375, 1379), (1380, 1383), (1384, 1391), (1392, 1397), (1398, 1404), (1405, 1409), (1410, 1413), (1414, 1418), (1419, 1423), (1424, 1433), (1433, 1435), (1436, 1444), (1445, 1448), (1449, 1455), (1456, 1458), (1459, 1461), (1462, 1472), (1473, 1475), (1476, 1481), (1482, 1487), (1487, 1488), (1489, 1498), (1499, 1504), (1505, 1515), (1516, 1519), (1520, 1526), (1527, 1537), (1537, 1541), (1542, 1547), (1548, 1553), (1554, 1557), (1558, 1566), (1567, 1569), (1570, 1574), (1575, 1579), (1580, 1586), (1587, 1589), (1590, 1595), (1595, 1596), (1597, 1601), (1601, 1602), (1603, 1605), (1606, 1610), (1610, 1611), (1612, 1618), (1619, 1627), (1628, 1631), (1632, 1637), (1638, 1644), (1645, 1647), (1648, 1653), (1654, 1656), (1657, 1658), (1659, 1671), (1672, 1679), (1680, 1683), (1684, 1689), (1690, 1697), (1698, 1701), (1702, 1706), (1707, 1712), (1713, 1723), (1724, 1726), (1727, 1732), (1732, 1733), (1734, 1742), (1743, 1751), (1752, 1755), (1756, 1759), (1760, 1764), (1765, 1771), (1772, 1775), (1776, 1784), (1785, 1790), (1791, 1796), (1797, 1800), (1801, 1805), (1806, 1810), (1811, 1815), (1816, 1826), (1826, 1827), (1828, 1831), (1832, 1834), (1835, 1838), (1839, 1843), (1844, 1849), (1850, 1858), (1859, 1862), (1863, 1867), (1868, 1869), (1870, 1878), (1879, 1881), (1882, 1884), (1885, 1889), (1890, 1893), (1893, 1895), (1896, 1899), (1900, 1906), (1907, 1909), (1910, 1913), (1914, 1916), (1917, 1925), (1926, 1936), (1937, 1941), (1942, 1945), (1946, 1954), (1955, 1957), (1958, 1964), (1965, 1970), (1971, 1975), (1976, 1981), (1982, 1988), (1989, 1991), (1992, 1997), (1998, 2002), (2002, 2003), (2004, 2009), (2010, 2014), (2015, 2018), (2019, 2030), (2031, 2039), (2039, 2040), (2041, 2046), (2047, 2050), (2051, 2055), (2056, 2058), (2059, 2065), (2066, 2074), (2074, 2075), (2075, 2076), (2076, 2077), (2077, 2080), (2081, 2083), (2084, 2087), (2088, 2095), (2096, 2103), (2104, 2111), (2112, 2115), (2116, 2122), (2123, 2125), (2126, 2133), (2134, 2136), (2137, 2143), (2144, 2146), (2147, 2154), (2155, 2163), (2164, 2168), (2169, 2173), (2174, 2180), (2181, 2185), (2186, 2189), (2190, 2192), (2193, 2197), (2198, 2202), (2203, 2207), (2208, 2214), (2215, 2223), (2224, 2226), (2227, 2233), (2234, 2238), (2239, 2245), (2246, 2250), (2251, 2253), (2254, 2260), (2261, 2268), (2269, 2274), (2275, 2285), (2286, 2294), (2294, 2295), (2296, 2299), (2300, 2309), (2310, 2316), (2317, 2319), (2320, 2322), (2323, 2330), (2330, 2331), (2332, 2337), (2338, 2342), (2343, 2346), (2347, 2349), (2350, 2354), (2355, 2357), (2358, 2362), (2363, 2365), (2366, 2373), (2374, 2377), (2378, 2380), (2381, 2386), (2387, 2394), (2395, 2402), (2403, 2407), (2408, 2413), (2413, 2414), (2415, 2418), (2419, 2425), (2426, 2429), (2430, 2435), (2436, 2440), (2441, 2444), (2445, 2450), (2451, 2461), (2462, 2465), (2466, 2470), (2471, 2473), (2474, 2481), (2482, 2486), (2487, 2491), (2492, 2495), (2496, 2499), (2499, 2500), (2501, 2503), (2504, 2512), (2513, 2521), (2522, 2524), (2525, 2527), (2528, 2536), (2537, 2545), (2545, 2546), (2547, 2551), (2552, 2557), (2558, 2560), (2561, 2571), (2572, 2574), (2575, 2585), (2586, 2593), (2594, 2597), (2598, 2606), (2607, 2610), (2611, 2614), (2615, 2619), (2620, 2623), (2624, 2627), (2628, 2637), (2638, 2641), (2642, 2649), (2650, 2654), (2655, 2658), (2659, 2667), (2668, 2672), (2673, 2675), (2676, 2683), (2684, 2686), (2687, 2693), (2694, 2699), (2700, 2707), (2708, 2712), (2713, 2717), (2718, 2720), (2721, 2725), (2726, 2729), (2730, 2734), (2735, 2738), (2739, 2742), (2743, 2746), (2747, 2749), (2750, 2754), (2755, 2762), (2763, 2766), (2767, 2773), (2774, 2776), (2777, 2784), (2785, 2792), (2793, 2799), (2800, 2802), (2803, 2808), (2809, 2814), (2815, 2818), (2819, 2822), (2823, 2827), (2828, 2832), (2833, 2836), (2837, 2841), (2842, 2848), (2848, 2849), (2850, 2852), (2853, 2861), (2861, 2862), (2863, 2867), (2868, 2871), (2872, 2876), (2877, 2878), (2879, 2886), (2887, 2889), (2890, 2898), (2899, 2902), (2903, 2907), (2908, 2914), (2915, 2918), (2919, 2927), (2928, 2935), (2936, 2938), (2939, 2943), (2944, 2951), (2952, 2954), (2955, 2961), (2962, 2966), (2967, 2969), (2970, 2976), (2976, 2977), (2978, 2986), (2986, 2993), (2993, 2994), (2995, 2998), (2999, 3003), (3004, 3008), (3009, 3015), (3016, 3022), (3023, 3029), (3029, 3030), (3031, 3033), (3034, 3039), (3040, 3048), (3049, 3053), (3054, 3058), (3059, 3062), (3063, 3069), (3070, 3074), (3075, 3077), (3078, 3081), (3082, 3086), (3087, 3093), (3094, 3098), (3099, 3103), (3104, 3110), (3111, 3113), (3114, 3120), (3121, 3123), (3124, 3127), (3128, 3132), (3133, 3136), (3137, 3141), (3142, 3148), (3149, 3152), (3153, 3157), (3158, 3160), (3161, 3165), (3166, 3169), (3170, 3175), (3176, 3178), (3179, 3187), (3188, 3193), (3194, 3198), (3199, 3202), (3203, 3210), (3211, 3215), (3215, 3216), (3217, 3221), (3222, 3225), (3226, 3234), (3235, 3241), (3242, 3244), (3245, 3246), (3247, 3255), (3256, 3258), (3259, 3262), (3263, 3269), (3270, 3272), (3273, 3277), (3278, 3282), (3283, 3291), (3292, 3295), (3296, 3299), (3300, 3302), (3303, 3309), (3309, 3310), (3310, 3311), (3311, 3312), (3312, 3316), (3317, 3320), (3321, 3327), (3328, 3336), (3337, 3345), (3346, 3354), (3355, 3358), (3359, 3362), (3363, 3367), (3368, 3376), (3377, 3380), (3381, 3383), (3384, 3387), (3388, 3394), (3395, 3400), (3401, 3405), (3406, 3410), (3411, 3415), (3416, 3428), (3428, 3429), (3430, 3433), (3434, 3443), (3444, 3445), (3446, 3451), (3452, 3461), (3461, 3462), (3463, 3470), (3470, 3471), (3472, 3477), (3478, 3484), (3485, 3495), (3496, 3500), (3500, 3504), (3505, 3515), (3516, 3520), (3521, 3522), (3523, 3527), (3528, 3537), (3537, 3538), (3539, 3544), (3545, 3550), (3551, 3559), (3560, 3562), (3563, 3568), (3569, 3572), (3573, 3576), (3577, 3587), (3588, 3592), (3593, 3596), (3597, 3599), (3600, 3606), (3607, 3612), (3613, 3621), (3621, 3622), (3623, 3628), (3629, 3634), (3635, 3639), (3640, 3644), (3645, 3647), (3648, 3653), (3654, 3665), (3666, 3674), (3674, 3675), (3676, 3682), (3682, 3683), (3684, 3692), (3693, 3698), (3699, 3707), (3708, 3716), (3717, 3721), (3722, 3725), (3725, 3727), (3728, 3732), (3733, 3735), (3736, 3741), (3742, 3747), (3748, 3755), (3756, 3768), (3769, 3771), (3772, 3780), (3781, 3784), (3785, 3790), (3791, 3795), (3796, 3798), (3799, 3804), (3805, 3808), (3809, 3812), (3813, 3823), (3824, 3826), (3827, 3829), (3830, 3841), (3842, 3846), (3847, 3851), (3852, 3857), (3858, 3863), (3864, 3872), (3873, 3878), (3878, 3879), (3880, 3887), (3888, 3893), (3894, 3900), (3901, 3904), (3905, 3911), (3912, 3920), (3921, 3929), (3930, 3938), (3939, 3942), (3943, 3947), (3948, 3950), (3951, 3955), (3956, 3958), (3959, 3967), (3968, 3976), (3977, 3980), (3981, 3987), (3988, 3990), (3991, 3996), (3997, 4001), (4002, 4006), (4007, 4011), (4012, 4016), (4017, 4021), (4022, 4026), (4027, 4032), (4033, 4040), (4041, 4044), (4045, 4053), (4053, 4054), (4055, 4060), (4061, 4063), (4064, 4068), (4069, 4074), (4075, 4079), (4080, 4083), (4084, 4086), (4087, 4091), (4091, 4092), (4093, 4096), (4097, 4103), (4104, 4107), (4108, 4113), (4114, 4118), (4119, 4124), (4125, 4134), (4135, 4139), (4140, 4145), (4146, 4149), (4150, 4156), (4157, 4159), (4160, 4168), (4169, 4177), (4178, 4181), (4182, 4185), (4186, 4190), (4191, 4197), (4198, 4201), (4202, 4207), (4208, 4212), (4213, 4216), (4217, 4219), (4220, 4224), (4225, 4231), (4232, 4236), (4237, 4244), (4245, 4253), (4254, 4262), (4262, 4263), (4264, 4266), (4267, 4273), (4274, 4282), (4283, 4287), (4288, 4295), (4296, 4300), (4301, 4306), (4307, 4316), (4317, 4324), (4325, 4333), (4334, 4342), (4343, 4346), (4347, 4353), (4354, 4357), (4358, 4362), (4363, 4365), (4366, 4373), (4374, 4377), (4378, 4383), (4384, 4388), (4389, 4398), (4399, 4401), (4402, 4409), (4410, 4415), (4416, 4425), (4426, 4431), (4432, 4434), (4435, 4440), (4441, 4450), (4450, 4451), (4451, 4452), (4452, 4453), (4453, 4459), (4460, 4468), (4469, 4472), (4473, 4479), (4480, 4482), (4483, 4487), (4488, 4496), (4497, 4505), (4506, 4511), (4512, 4514), (4515, 4524), (4525, 4535), (4536, 4538), (4539, 4542), (4543, 4547), (4548, 4551), (4552, 4563), (4564, 4567), (4568, 4574), (4575, 4577), (4578, 4581), (4582, 4584), (4584, 4585), (4586, 4589), (4590, 4592), (4593, 4596), (4597, 4601), (4602, 4604), (4605, 4615), (4616, 4619), (4620, 4625), (4626, 4629), (4630, 4634), (4635, 4639), (4640, 4644), (4645, 4648), (4649, 4651), (4652, 4658), (4659, 4669), (4670, 4674), (4675, 4676), (4677, 4684), (4684, 4685), (4686, 4688), (4689, 4691), (4692, 4694), (4695, 4709), (4710, 4720), (4721, 4725), (4726, 4730), (4731, 4735), (4736, 4738), (4739, 4744), (4745, 4748), (4749, 4756), (4757, 4765), (4766, 4770), (4771, 4775), (4776, 4780), (4781, 4789), (4790, 4793), (4794, 4802), (4802, 4803), (4804, 4807), (4808, 4812), (4813, 4815), (4816, 4819), (4820, 4830), (4831, 4834), (4835, 4839), (4839, 4840), (4841, 4847), (4848, 4852), (4853, 4862), (4863, 4868), (4869, 4875), (4876, 4883), (4884, 4887), (4888, 4893), (4894, 4900), (4901, 4905), (4906, 4914), (4915, 4917), (4918, 4926), (4927, 4930), (4931, 4936), (4937, 4942), (4943, 4946), (4947, 4957), (4958, 4963), (4964, 4967), (4968, 4976), (4977, 4981), (4982, 4985), (4986, 4992), (4993, 4997), (4998, 5003), (5004, 5006), (5007, 5014), (5015, 5023), (5024, 5026), (5027, 5039), (5040, 5046), (5047, 5051), (5052, 5054), (5055, 5063), (5063, 5064), (5065, 5071), (5072, 5076), (5077, 5080), (5081, 5083), (5084, 5091), (5092, 5098), (5099, 5102), (5103, 5108), (5109, 5111), (5112, 5120), (5121, 5127), (5128, 5130), (5131, 5135), (5136, 5140), (5141, 5147), (5148, 5156), (5157, 5167), (5168, 5177), (5178, 5181), (5182, 5188), (5189, 5191), (5192, 5196), (5197, 5203), (5204, 5211), (5211, 5212), (5213, 5216), (5217, 5223), (5224, 5226), (5227, 5235), (5236, 5244), (5245, 5248), (5249, 5252), (5253, 5256), (5257, 5262), (5263, 5265), (5266, 5273), (5274, 5278), (5279, 5283), (5284, 5286), (5287, 5294), (5294, 5295), (5296, 5303), (5303, 5304), (5305, 5308), (5309, 5313), (5314, 5317), (5318, 5320), (5321, 5326), (5327, 5330), (5331, 5333), (5334, 5342), (5343, 5351), (5352, 5357), (5358, 5360), (5361, 5364), (5365, 5367), (5368, 5370), (5371, 5379), (5380, 5389), (5390, 5392), (5393, 5395), (5396, 5403), (5403, 5404), (5405, 5409), (5410, 5413), (5414, 5418), (5419, 5420), (5421, 5425), (5425, 5426), (5427, 5429), (5430, 5436), (5437, 5440), (5441, 5445), (5446, 5449), (5450, 5456), (5457, 5464), (5465, 5467), (5468, 5471), (5472, 5474), (5475, 5481), (5482, 5484), (0, 0)], 'labels': [15, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_tokenize_assign_labels(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0310c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "500f8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "for x in datasets['train']:\n",
    "    if x['id'] == '7D95539DC1AE':\n",
    "        temp = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4f2c0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = tokenizer(temp['raw_text'],truncation=True, padding=True,\n",
    "                      max_length=1024, return_offsets_mapping=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2764effa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 497, 78, 802, 6, 4472, 2239, 189, 2045, 101, 41, 1365, 169, 66, 13, 1159, 54, 95, 236, 7, 14514, 334, 7, 45, 28, 547, 8943, 13, 24, 131, 959, 6, 150, 24, 189, 2045, 3231, 12, 38459, 7, 1157, 521, 7, 2845, 7, 185, 49, 4050, 804, 6, 24, 64, 747, 498, 28, 182, 10142, 13, 258, 167, 54, 185, 2093, 9, 5, 1973, 8, 167, 54, 535, 10, 2065, 2718, 9, 334, 4, 870, 2455, 521, 5, 1973, 7, 2807, 49, 2239, 1737, 172, 24, 8382, 521, 2845, 549, 51, 1532, 275, 31, 5, 5863, 9, 49, 308, 184, 6, 10, 8171, 6, 50, 4558, 1493, 4, 96, 1285, 6, 1311, 521, 5, 1973, 7, 2845, 147, 8, 141, 51, 1532, 275, 2386, 55, 4736, 521, 7, 1095, 184, 1195, 87, 283, 7, 334, 142, 51, 351, 75, 33, 7, 4022, 59, 3064, 639, 15, 334, 6014, 4, 993, 82, 189, 206, 4472, 2239, 16, 95, 10, 169, 13, 1159, 7, 185, 41, 1365, 169, 66, 9, 49, 1265, 6, 1622, 142, 51, 32, 22414, 131, 77, 6, 11, 754, 6, 4472, 2239, 16, 45, 129, 10, 169, 13, 408, 7, 676, 430, 1380, 11019, 6, 53, 24, 16, 67, 10, 169, 13, 521, 7, 109, 49, 233, 11, 4881, 5, 2504, 9, 16759, 26092, 6357, 4, 50118, 50118, 243, 16, 10, 1537, 6563, 14, 5, 275, 1265, 606, 31, 1025, 10, 8171, 8, 97, 839, 9, 1265, 32, 7421, 13514, 87, 10, 8171, 1265, 4, 152, 189, 28, 1528, 13, 171, 82, 53, 24, 16, 2299, 45, 1528, 13, 961, 4, 4210, 16943, 32, 747, 7337, 8, 11138, 61, 2386, 190, 5, 144, 157, 6589, 196, 521, 5, 778, 7, 28, 16573, 30, 49, 6763, 4, 7411, 209, 15083, 32, 818, 37580, 868, 61, 3315, 5, 2948, 7, 912, 190, 667, 7, 6396, 4, 152, 6, 11, 1004, 6, 3607, 521, 54, 3127, 770, 7, 1532, 23, 10, 22025, 142, 122, 49, 3254, 34, 685, 49, 10563, 7, 6396, 4, 40438, 2239, 64, 45, 129, 1888, 5, 2166, 1380, 10070, 8, 146, 106, 55, 25832, 6, 53, 24, 64, 67, 1157, 521, 54, 33, 10, 16699, 7, 28, 55, 3236, 7180, 5, 1973, 7, 192, 114, 8201, 1235, 31, 5, 2621, 9, 82, 2607, 106, 1056, 357, 15, 49, 173, 4, 14747, 31, 5, 5984, 1795, 6, 89, 64, 67, 28, 474, 1795, 4, 50118, 50118, 3762, 9, 5, 934, 2188, 5467, 64, 2504, 98, 1335, 11, 334, 16, 142, 521, 619, 14, 3867, 51, 32, 98, 4736, 14, 51, 1395, 5043, 11, 334, 51, 197, 283, 7, 334, 142, 49, 6856, 25473, 29, 5, 801, 2504, 9, 41, 5467, 4, 616, 51, 189, 28, 441, 7, 146, 24, 149, 70, 9, 49, 4050, 396, 203, 696, 6, 5, 6976, 122, 5712, 2500, 70, 49, 18295, 54, 58, 11, 1511, 19, 106, 70, 183, 4, 870, 2455, 521, 7, 109, 4472, 2239, 6, 42, 696, 9, 6856, 16, 9820, 142, 5, 521, 54, 32, 4736, 53, 45, 3606, 350, 4008, 31, 5, 5298, 40, 28, 1220, 7, 2725, 49, 4050, 31, 184, 98, 51, 64, 185, 5, 183, 160, 7, 244, 2097, 5, 2504, 9, 5467, 396, 519, 7, 4022, 59, 70, 5, 173, 51, 189, 33, 2039, 4, 96, 1285, 6, 42, 64, 244, 10, 3143, 9, 521, 54, 2649, 334, 13, 3112, 5788, 9, 86, 142, 9, 743, 215, 25, 1356, 6, 1098, 1938, 6, 8, 190, 103, 2536, 474, 743, 4, 318, 209, 521, 216, 51, 64, 2773, 146, 62, 5, 173, 137, 51, 190, 671, 7, 334, 24, 64, 185, 65, 540, 3992, 160, 106, 98, 51, 64, 1056, 15, 3046, 696, 51, 32, 4098, 19, 4, 152, 189, 888, 898, 11, 10, 7280, 9, 5, 5933, 9, 86, 14, 521, 32, 66, 9, 334, 4, 50118, 50118, 10787, 54, 9449, 2455, 4472, 2239, 189, 224, 14, 521, 54, 109, 45, 2725, 1380, 40, 33, 55, 26434, 6, 8, 3891, 10, 3007, 1265, 131, 959, 6, 209, 82, 26506, 7856, 19593, 6856, 19, 10, 205, 1265, 4, 17482, 498, 521, 11, 1380, 54, 32, 16573, 40, 253, 62, 602, 97, 521, 108, 1056, 552, 19, 106, 25, 49, 17447, 21943, 29, 643, 4, 6983, 608, 4472, 2239, 51, 351, 75, 33, 7, 4022, 59, 751, 26434, 98, 521, 54, 3127, 236, 7, 1056, 64, 342, 1235, 11, 41, 1737, 14, 275, 10401, 49, 2239, 2496, 4, 2044, 477, 82, 54, 9449, 17679, 4472, 2239, 189, 146, 16, 14, 30, 2455, 521, 5, 2031, 7, 1532, 31, 184, 171, 40, 95, 912, 49, 3218, 70, 561, 4, 616, 11, 103, 1200, 42, 189, 28, 1528, 6, 5, 82, 54, 74, 912, 49, 1265, 77, 576, 5, 1973, 9, 4472, 2239, 32, 5, 276, 82, 54, 74, 1874, 66, 9, 239, 334, 190, 396, 4472, 2239, 4, 870, 1311, 521, 55, 797, 81, 49, 1265, 149, 4472, 2239, 5, 82, 54, 236, 7, 6726, 32, 576, 55, 1915, 7, 617, 49, 1265, 61, 16, 3127, 505, 4, 50118, 50118, 39173, 521, 5, 1973, 7, 33, 4472, 2239, 115, 28, 2778, 10142, 7, 45, 95, 5, 2172, 54, 2845, 7, 860, 24, 6, 53, 24, 64, 67, 28, 10142, 13, 167, 54, 619, 51, 240, 5, 11, 621, 676, 19, 10, 3254, 4, 85, 16, 41, 19717, 15480, 14, 77, 314, 7, 49, 308, 2110, 521, 40, 95, 6602, 2239, 70, 561, 6, 53, 42, 16, 45, 5705, 5, 403, 4, 1590, 171, 7576, 147, 334, 1743, 32, 145, 373, 88, 864, 50, 1986, 32, 145, 355, 5, 1756, 817, 5, 4795, 14, 5, 464, 40, 1303, 41, 1374, 7280, 9, 8106, 1195, 87, 41, 712, 4, 3791, 42, 189, 28, 142, 82, 32, 16292, 7, 9375, 464, 98, 95, 101, 1311, 521, 1736, 7796, 34, 1006, 11, 171, 334, 1743, 6, 5, 1973, 9, 4472, 2239, 189, 65, 183, 3364, 7, 1477, 55, 87, 24, 8357, 4, 635, 6, 5, 129, 169, 7, 3127, 192, 114, 4472, 2239, 1364, 50, 45, 16, 7, 888, 5731, 24, 11, 1304, 6, 190, 13, 95, 10, 76, 6, 7, 7118, 5, 414, 8, 2845, 549, 50, 45, 24, 197, 28, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 8), (9, 16), (16, 17), (18, 26), (27, 35), (36, 39), (40, 44), (45, 49), (50, 52), (53, 57), (58, 61), (62, 65), (66, 69), (70, 74), (75, 78), (79, 83), (84, 88), (89, 91), (92, 96), (97, 103), (104, 106), (107, 110), (111, 113), (114, 118), (119, 130), (131, 134), (135, 137), (137, 138), (139, 146), (146, 147), (148, 153), (154, 156), (157, 160), (161, 165), (166, 173), (173, 174), (174, 183), (184, 186), (187, 192), (193, 201), (202, 204), (205, 211), (212, 214), (215, 219), (220, 225), (226, 233), (234, 240), (240, 241), (242, 244), (245, 248), (249, 254), (255, 260), (261, 263), (264, 268), (269, 279), (280, 283), (284, 288), (289, 294), (295, 298), (299, 303), (304, 313), (314, 316), (317, 320), (321, 327), (328, 331), (332, 337), (338, 341), (342, 350), (351, 352), (353, 364), (365, 369), (370, 372), (373, 379), (379, 380), (381, 383), (384, 392), (393, 401), (402, 405), (406, 412), (413, 415), (416, 422), (423, 428), (429, 437), (438, 449), (450, 454), (455, 457), (458, 462), (463, 471), (472, 478), (479, 486), (487, 491), (492, 497), (498, 502), (503, 507), (508, 511), (512, 519), (520, 522), (523, 528), (529, 532), (533, 537), (537, 538), (539, 540), (541, 550), (550, 551), (552, 554), (555, 563), (564, 568), (568, 569), (570, 572), (573, 581), (581, 582), (583, 589), (590, 598), (599, 602), (603, 609), (610, 612), (613, 619), (620, 625), (626, 629), (630, 633), (634, 638), (639, 644), (645, 649), (650, 656), (657, 661), (662, 666), (667, 675), (676, 678), (679, 683), (684, 688), (689, 695), (696, 700), (701, 705), (706, 708), (709, 715), (716, 723), (724, 728), (729, 732), (732, 734), (735, 739), (740, 742), (743, 748), (749, 754), (755, 762), (763, 769), (770, 772), (773, 779), (779, 783), (783, 784), (785, 789), (790, 796), (797, 800), (801, 806), (807, 815), (816, 824), (825, 827), (828, 832), (833, 834), (835, 838), (839, 842), (843, 847), (848, 850), (851, 855), (856, 858), (859, 863), (864, 867), (868, 871), (872, 874), (875, 880), (881, 890), (890, 891), (892, 898), (899, 906), (907, 911), (912, 915), (916, 920), (920, 921), (922, 926), (926, 927), (928, 930), (931, 935), (935, 936), (937, 945), (946, 954), (955, 957), (958, 961), (962, 966), (967, 968), (969, 972), (973, 976), (977, 985), (986, 988), (989, 999), (1000, 1009), (1010, 1015), (1016, 1022), (1022, 1023), (1024, 1027), (1028, 1030), (1031, 1033), (1034, 1038), (1039, 1040), (1041, 1044), (1045, 1048), (1049, 1057), (1058, 1060), (1061, 1063), (1064, 1069), (1070, 1074), (1075, 1077), (1078, 1086), (1087, 1090), (1091, 1097), (1098, 1100), (1101, 1107), (1107, 1113), (1114, 1122), (1122, 1123), (1123, 1124), (1124, 1125), (1125, 1127), (1128, 1130), (1131, 1132), (1133, 1139), (1140, 1146), (1147, 1151), (1152, 1155), (1156, 1160), (1161, 1170), (1171, 1176), (1177, 1181), (1182, 1188), (1189, 1190), (1191, 1200), (1201, 1204), (1205, 1210), (1211, 1216), (1217, 1219), (1220, 1229), (1230, 1233), (1234, 1241), (1242, 1248), (1249, 1253), (1254, 1255), (1256, 1265), (1266, 1275), (1275, 1276), (1277, 1281), (1282, 1285), (1286, 1288), (1289, 1293), (1294, 1297), (1298, 1302), (1303, 1309), (1310, 1313), (1314, 1316), (1317, 1319), (1320, 1330), (1331, 1334), (1335, 1339), (1340, 1343), (1344, 1352), (1352, 1353), (1354, 1359), (1359, 1364), (1365, 1368), (1369, 1374), (1375, 1379), (1380, 1383), (1384, 1391), (1392, 1397), (1398, 1404), (1405, 1409), (1410, 1413), (1414, 1418), (1419, 1423), (1424, 1433), (1433, 1435), (1436, 1444), (1445, 1448), (1449, 1455), (1456, 1458), (1459, 1461), (1462, 1472), (1473, 1475), (1476, 1481), (1482, 1487), (1487, 1488), (1489, 1498), (1499, 1504), (1505, 1515), (1516, 1519), (1520, 1526), (1527, 1537), (1537, 1541), (1542, 1547), (1548, 1553), (1554, 1557), (1558, 1566), (1567, 1569), (1570, 1574), (1575, 1579), (1580, 1586), (1587, 1589), (1590, 1595), (1595, 1596), (1597, 1601), (1601, 1602), (1603, 1605), (1606, 1610), (1610, 1611), (1612, 1618), (1619, 1627), (1628, 1631), (1632, 1637), (1638, 1644), (1645, 1647), (1648, 1653), (1654, 1656), (1657, 1658), (1659, 1671), (1672, 1679), (1680, 1683), (1684, 1689), (1690, 1697), (1698, 1701), (1702, 1706), (1707, 1712), (1713, 1723), (1724, 1726), (1727, 1732), (1732, 1733), (1734, 1742), (1743, 1751), (1752, 1755), (1756, 1759), (1760, 1764), (1765, 1771), (1772, 1775), (1776, 1784), (1785, 1790), (1791, 1796), (1797, 1800), (1801, 1805), (1806, 1810), (1811, 1815), (1816, 1826), (1826, 1827), (1828, 1831), (1832, 1834), (1835, 1838), (1839, 1843), (1844, 1849), (1850, 1858), (1859, 1862), (1863, 1867), (1868, 1869), (1870, 1878), (1879, 1881), (1882, 1884), (1885, 1889), (1890, 1893), (1893, 1895), (1896, 1899), (1900, 1906), (1907, 1909), (1910, 1913), (1914, 1916), (1917, 1925), (1926, 1936), (1937, 1941), (1942, 1945), (1946, 1954), (1955, 1957), (1958, 1964), (1965, 1970), (1971, 1975), (1976, 1981), (1982, 1988), (1989, 1991), (1992, 1997), (1998, 2002), (2002, 2003), (2004, 2009), (2010, 2014), (2015, 2018), (2019, 2030), (2031, 2039), (2039, 2040), (2041, 2046), (2047, 2050), (2051, 2055), (2056, 2058), (2059, 2065), (2066, 2074), (2074, 2075), (2075, 2076), (2076, 2077), (2077, 2080), (2081, 2083), (2084, 2087), (2088, 2095), (2096, 2103), (2104, 2111), (2112, 2115), (2116, 2122), (2123, 2125), (2126, 2133), (2134, 2136), (2137, 2143), (2144, 2146), (2147, 2154), (2155, 2163), (2164, 2168), (2169, 2173), (2174, 2180), (2181, 2185), (2186, 2189), (2190, 2192), (2193, 2197), (2198, 2202), (2203, 2207), (2208, 2214), (2215, 2223), (2224, 2226), (2227, 2233), (2234, 2238), (2239, 2245), (2246, 2250), (2251, 2253), (2254, 2260), (2261, 2268), (2269, 2274), (2275, 2285), (2286, 2294), (2294, 2295), (2296, 2299), (2300, 2309), (2310, 2316), (2317, 2319), (2320, 2322), (2323, 2330), (2330, 2331), (2332, 2337), (2338, 2342), (2343, 2346), (2347, 2349), (2350, 2354), (2355, 2357), (2358, 2362), (2363, 2365), (2366, 2373), (2374, 2377), (2378, 2380), (2381, 2386), (2387, 2394), (2395, 2402), (2403, 2407), (2408, 2413), (2413, 2414), (2415, 2418), (2419, 2425), (2426, 2429), (2430, 2435), (2436, 2440), (2441, 2444), (2445, 2450), (2451, 2461), (2462, 2465), (2466, 2470), (2471, 2473), (2474, 2481), (2482, 2486), (2487, 2491), (2492, 2495), (2496, 2499), (2499, 2500), (2501, 2503), (2504, 2512), (2513, 2521), (2522, 2524), (2525, 2527), (2528, 2536), (2537, 2545), (2545, 2546), (2547, 2551), (2552, 2557), (2558, 2560), (2561, 2571), (2572, 2574), (2575, 2585), (2586, 2593), (2594, 2597), (2598, 2606), (2607, 2610), (2611, 2614), (2615, 2619), (2620, 2623), (2624, 2627), (2628, 2637), (2638, 2641), (2642, 2649), (2650, 2654), (2655, 2658), (2659, 2667), (2668, 2672), (2673, 2675), (2676, 2683), (2684, 2686), (2687, 2693), (2694, 2699), (2700, 2707), (2708, 2712), (2713, 2717), (2718, 2720), (2721, 2725), (2726, 2729), (2730, 2734), (2735, 2738), (2739, 2742), (2743, 2746), (2747, 2749), (2750, 2754), (2755, 2762), (2763, 2766), (2767, 2773), (2774, 2776), (2777, 2784), (2785, 2792), (2793, 2799), (2800, 2802), (2803, 2808), (2809, 2814), (2815, 2818), (2819, 2822), (2823, 2827), (2828, 2832), (2833, 2836), (2837, 2841), (2842, 2848), (2848, 2849), (2850, 2852), (2853, 2861), (2861, 2862), (2863, 2867), (2868, 2871), (2872, 2876), (2877, 2878), (2879, 2886), (2887, 2889), (2890, 2898), (2899, 2902), (2903, 2907), (2908, 2914), (2915, 2918), (2919, 2927), (2928, 2935), (2936, 2938), (2939, 2943), (2944, 2951), (2952, 2954), (2955, 2961), (2962, 2966), (2967, 2969), (2970, 2976), (2976, 2977), (2978, 2986), (2986, 2993), (2993, 2994), (2995, 2998), (2999, 3003), (3004, 3008), (3009, 3015), (3016, 3022), (3023, 3029), (3029, 3030), (3031, 3033), (3034, 3039), (3040, 3048), (3049, 3053), (3054, 3058), (3059, 3062), (3063, 3069), (3070, 3074), (3075, 3077), (3078, 3081), (3082, 3086), (3087, 3093), (3094, 3098), (3099, 3103), (3104, 3110), (3111, 3113), (3114, 3120), (3121, 3123), (3124, 3127), (3128, 3132), (3133, 3136), (3137, 3141), (3142, 3148), (3149, 3152), (3153, 3157), (3158, 3160), (3161, 3165), (3166, 3169), (3170, 3175), (3176, 3178), (3179, 3187), (3188, 3193), (3194, 3198), (3199, 3202), (3203, 3210), (3211, 3215), (3215, 3216), (3217, 3221), (3222, 3225), (3226, 3234), (3235, 3241), (3242, 3244), (3245, 3246), (3247, 3255), (3256, 3258), (3259, 3262), (3263, 3269), (3270, 3272), (3273, 3277), (3278, 3282), (3283, 3291), (3292, 3295), (3296, 3299), (3300, 3302), (3303, 3309), (3309, 3310), (3310, 3311), (3311, 3312), (3312, 3316), (3317, 3320), (3321, 3327), (3328, 3336), (3337, 3345), (3346, 3354), (3355, 3358), (3359, 3362), (3363, 3367), (3368, 3376), (3377, 3380), (3381, 3383), (3384, 3387), (3388, 3394), (3395, 3400), (3401, 3405), (3406, 3410), (3411, 3415), (3416, 3428), (3428, 3429), (3430, 3433), (3434, 3443), (3444, 3445), (3446, 3451), (3452, 3461), (3461, 3462), (3463, 3470), (3470, 3471), (3472, 3477), (3478, 3484), (3485, 3495), (3496, 3500), (3500, 3504), (3505, 3515), (3516, 3520), (3521, 3522), (3523, 3527), (3528, 3537), (3537, 3538), (3539, 3544), (3545, 3550), (3551, 3559), (3560, 3562), (3563, 3568), (3569, 3572), (3573, 3576), (3577, 3587), (3588, 3592), (3593, 3596), (3597, 3599), (3600, 3606), (3607, 3612), (3613, 3621), (3621, 3622), (3623, 3628), (3629, 3634), (3635, 3639), (3640, 3644), (3645, 3647), (3648, 3653), (3654, 3665), (3666, 3674), (3674, 3675), (3676, 3682), (3682, 3683), (3684, 3692), (3693, 3698), (3699, 3707), (3708, 3716), (3717, 3721), (3722, 3725), (3725, 3727), (3728, 3732), (3733, 3735), (3736, 3741), (3742, 3747), (3748, 3755), (3756, 3768), (3769, 3771), (3772, 3780), (3781, 3784), (3785, 3790), (3791, 3795), (3796, 3798), (3799, 3804), (3805, 3808), (3809, 3812), (3813, 3823), (3824, 3826), (3827, 3829), (3830, 3841), (3842, 3846), (3847, 3851), (3852, 3857), (3858, 3863), (3864, 3872), (3873, 3878), (3878, 3879), (3880, 3887), (3888, 3893), (3894, 3900), (3901, 3904), (3905, 3911), (3912, 3920), (3921, 3929), (3930, 3938), (3939, 3942), (3943, 3947), (3948, 3950), (3951, 3955), (3956, 3958), (3959, 3967), (3968, 3976), (3977, 3980), (3981, 3987), (3988, 3990), (3991, 3996), (3997, 4001), (4002, 4006), (4007, 4011), (4012, 4016), (4017, 4021), (4022, 4026), (4027, 4032), (4033, 4040), (4041, 4044), (4045, 4053), (4053, 4054), (4055, 4060), (4061, 4063), (4064, 4068), (4069, 4074), (4075, 4079), (4080, 4083), (4084, 4086), (4087, 4091), (4091, 4092), (4093, 4096), (4097, 4103), (4104, 4107), (4108, 4113), (4114, 4118), (4119, 4124), (4125, 4134), (4135, 4139), (4140, 4145), (4146, 4149), (4150, 4156), (4157, 4159), (4160, 4168), (4169, 4177), (4178, 4181), (4182, 4185), (4186, 4190), (4191, 4197), (4198, 4201), (4202, 4207), (4208, 4212), (4213, 4216), (4217, 4219), (4220, 4224), (4225, 4231), (4232, 4236), (4237, 4244), (4245, 4253), (4254, 4262), (4262, 4263), (4264, 4266), (4267, 4273), (4274, 4282), (4283, 4287), (4288, 4295), (4296, 4300), (4301, 4306), (4307, 4316), (4317, 4324), (4325, 4333), (4334, 4342), (4343, 4346), (4347, 4353), (4354, 4357), (4358, 4362), (4363, 4365), (4366, 4373), (4374, 4377), (4378, 4383), (4384, 4388), (4389, 4398), (4399, 4401), (4402, 4409), (4410, 4415), (4416, 4425), (4426, 4431), (4432, 4434), (4435, 4440), (4441, 4450), (4450, 4451), (4451, 4452), (4452, 4453), (4453, 4459), (4460, 4468), (4469, 4472), (4473, 4479), (4480, 4482), (4483, 4487), (4488, 4496), (4497, 4505), (4506, 4511), (4512, 4514), (4515, 4524), (4525, 4535), (4536, 4538), (4539, 4542), (4543, 4547), (4548, 4551), (4552, 4563), (4564, 4567), (4568, 4574), (4575, 4577), (4578, 4581), (4582, 4584), (4584, 4585), (4586, 4589), (4590, 4592), (4593, 4596), (4597, 4601), (4602, 4604), (4605, 4615), (4616, 4619), (4620, 4625), (4626, 4629), (4630, 4634), (4635, 4639), (4640, 4644), (4645, 4648), (4649, 4651), (4652, 4658), (4659, 4669), (4670, 4674), (4675, 4676), (4677, 4684), (4684, 4685), (4686, 4688), (4689, 4691), (4692, 4694), (4695, 4709), (4710, 4720), (4721, 4725), (4726, 4730), (4731, 4735), (4736, 4738), (4739, 4744), (4745, 4748), (4749, 4756), (4757, 4765), (4766, 4770), (4771, 4775), (4776, 4780), (4781, 4789), (4790, 4793), (4794, 4802), (4802, 4803), (4804, 4807), (4808, 4812), (4813, 4815), (4816, 4819), (4820, 4830), (4831, 4834), (4835, 4839), (4839, 4840), (4841, 4847), (4848, 4852), (4853, 4862), (4863, 4868), (4869, 4875), (4876, 4883), (4884, 4887), (4888, 4893), (4894, 4900), (4901, 4905), (4906, 4914), (4915, 4917), (4918, 4926), (4927, 4930), (4931, 4936), (4937, 4942), (4943, 4946), (4947, 4957), (4958, 4963), (4964, 4967), (4968, 4976), (4977, 4981), (4982, 4985), (4986, 4992), (4993, 4997), (4998, 5003), (5004, 5006), (5007, 5014), (5015, 5023), (5024, 5026), (5027, 5039), (5040, 5046), (5047, 5051), (5052, 5054), (5055, 5063), (5063, 5064), (5065, 5071), (5072, 5076), (5077, 5080), (5081, 5083), (5084, 5091), (5092, 5098), (5099, 5102), (5103, 5108), (5109, 5111), (5112, 5120), (5121, 5127), (5128, 5130), (5131, 5135), (5136, 5140), (5141, 5147), (5148, 5156), (5157, 5167), (5168, 5177), (5178, 5181), (5182, 5188), (5189, 5191), (5192, 5196), (5197, 5203), (5204, 5211), (5211, 5212), (5213, 5216), (5217, 5223), (5224, 5226), (5227, 5235), (5236, 5244), (5245, 5248), (5249, 5252), (5253, 5256), (5257, 5262), (5263, 5265), (5266, 5273), (5274, 5278), (5279, 5283), (5284, 5286), (5287, 5294), (5294, 5295), (5296, 5303), (5303, 5304), (5305, 5308), (5309, 5313), (5314, 5317), (5318, 5320), (5321, 5326), (5327, 5330), (5331, 5333), (5334, 5342), (5343, 5351), (5352, 5357), (5358, 5360), (5361, 5364), (5365, 5367), (5368, 5370), (5371, 5379), (5380, 5389), (5390, 5392), (5393, 5395), (5396, 5403), (5403, 5404), (5405, 5409), (5410, 5413), (5414, 5418), (5419, 5420), (5421, 5425), (5425, 5426), (5427, 5429), (5430, 5436), (5437, 5440), (5441, 5445), (5446, 5449), (5450, 5456), (5457, 5464), (5465, 5467), (5468, 5471), (5472, 5474), (5475, 5481), (5482, 5484), (0, 0)]}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5630c6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check['offset_mapping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dfab7bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7D95539DC1AE\n",
      "1024\n",
      "5495.0\n",
      "I-Concluding Statement\n",
      "[15, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "test_input = tokenize_assign_labels(temp)\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c6799c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7D95539DC1AE',\n",
       " 'discourse_start': [0.0,\n",
       "  381.0,\n",
       "  570.0,\n",
       "  1125.0,\n",
       "  1277.0,\n",
       "  2077.0,\n",
       "  2332.0,\n",
       "  3312.0,\n",
       "  3539.0,\n",
       "  3880.0,\n",
       "  4055.0,\n",
       "  4453.0],\n",
       " 'discourse_end': [380.0,\n",
       "  570.0,\n",
       "  1124.0,\n",
       "  1277.0,\n",
       "  2076.0,\n",
       "  2331.0,\n",
       "  3311.0,\n",
       "  3538.0,\n",
       "  3879.0,\n",
       "  4054.0,\n",
       "  4452.0,\n",
       "  5495.0],\n",
       " 'discourse_type': ['Lead',\n",
       "  'Position',\n",
       "  'Evidence',\n",
       "  'Claim',\n",
       "  'Evidence',\n",
       "  'Claim',\n",
       "  'Evidence',\n",
       "  'Claim',\n",
       "  'Evidence',\n",
       "  'Claim',\n",
       "  'Evidence',\n",
       "  'Concluding Statement'],\n",
       " 'raw_text': \"At first thought, distance learning may seem like an easy way out for kids who just want to skip school to not be held accountable for it; however, while it may seem counter-intuitive to allow students to decide to take their classes online, it can often times be very beneficial for both those who take advantage of the option and those who continue a traditional path of school. By allowing students the option to choose their learning environment then it lets students decide whether they learn best from the comfort of their own home, a classroom, or anywhere else. In addition, giving students the option to decide where and how they learn best allows more sick students to stay home rather than come to school because they won't have to worry about falling behind on schoolwork. Some people may think distance learning is just a way for kids to take an easy way out of their education, simply because they are lazy; when, in fact, distance learning is not only a way for children to experience different class styles, but it is also a way for students to do their part in reducing the spread of communicable diseases.\\n\\nIt is a common belief that the best education comes from inside a classroom and other means of education are somehow lesser than a classroom education. This may be true for many people but it is definitely not true for everyone. Classrooms are often loud and crowded which allows even the most well intentioned students the chance to be distracted by their peers. Sometimes these classrooms are almost uncontrollable which leads the teachers to stop even trying to teach. This, in turn, leaves students who truly wanted to learn at a disadvantage because now their teacher has lost their motivation to teach. Distance learning can not only reduce the physical class sizes and make them more manageable, but it can also allow students who have a tendency to be more rowdy the option to see if removing themselves from the presence of people helps them focus better on their work. Aside from the educational benefits, there can also be health benefits.\\n\\nOne of the biggest reasons illness can spread so quickly in school is because students feel that unless they are so sick that they cannot function in school they should come to school because their attendance outweighs the potential spread of an illness. While they may be able to make it through all of their classes without much issue, the burden now falls onto all their classmates who were in contact with them all day. By allowing students to do distance learning, this issue of attendance is eliminated because the students who are sick but not suffering too heavily from the symptoms will be allowed to attend their classes from home so they can take the day off to help prevent the spread of illness without having to worry about all the work they may have missed. In addition, this can help a variety of students who miss school for extended periods of time because of issues such as injury, hospitalization, and even some mental health issues. If these students know they can easily make up the work before they even return to school it can take one less stress off them so they can focus on whatever issue they are dealing with. This may actually result in a decrease of the length of time that students are out of school.\\n\\nMany who oppose allowing distance learning may say that students who do not attend class will have more distractions, and therefore a worse education; however, these people mistakenly conflate attendance with a good education. Often times students in class who are distracted will end up taking other students' focus along with them as their distraction distracts others. Students doing distance learning they won't have to worry about outside distractions so students who truly want to focus can put themselves in an environment that best suits their learning style. Another point people who oppose optional distance learning may make is that by allowing students the choice to learn from home many will just stop their studies all together. While in some cases this may be true, the people who would stop their education when given the option of distance learning are the same people who would drop out of high school even without distance learning. By giving students more control over their education through distance learning the people who want to succeed are given more resources to further their education which is truly important.\\n\\nGiving students the option to have distance learning could be extremely beneficial to not just the individuals who decide to try it, but it can also be beneficial for those who feel they need the in person experience with a teacher. It is an understandable assumption that when left to their own devices students will just quit learning all together, but this is not frequently the case. During many arguments where school systems are being called into question or policies are being added the opposition makes the argument that the change will cause an overall decrease of productivity rather than an increase. Though this may be because people are prone to opposing change so just like giving students individual computers has worked in many school systems, the option of distance learning may one day prove to improve more than it damages. However, the only way to truly see if distance learning works or not is to actually implement it in schools, even for just a year, to assess the data and decide whether or not it should be continued.\",\n",
       " '__index_level_0__': 7621}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cc0bb212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m0/r4gc8d8x2dz_h2ymv3jgxbv80000gn/T/ipykernel_39840/674512480.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(test_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "13cf0ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 0,\n",
       " 'B-Lead': 1,\n",
       " 'I-Lead': 2,\n",
       " 'B-Position': 3,\n",
       " 'I-Position': 4,\n",
       " 'B-Evidence': 5,\n",
       " 'I-Evidence': 6,\n",
       " 'B-Claim': 7,\n",
       " 'I-Claim': 8,\n",
       " 'B-Concluding Statement': 9,\n",
       " 'I-Concluding Statement': 10,\n",
       " 'B-Counterclaim': 11,\n",
       " 'I-Counterclaim': 12,\n",
       " 'B-Rebuttal': 13,\n",
       " 'I-Rebuttal': 14,\n",
       " 'special': 15}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "319c3782",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>idx_label</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117053</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621712e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "      <td>(Lead, 0.0, 380.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117054</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621712e+12</td>\n",
       "      <td>381.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>By allowing students the option to choose thei...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 8...</td>\n",
       "      <td>(Position, 381.0, 570.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117055</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621712e+12</td>\n",
       "      <td>570.0</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>In addition, giving students the option to dec...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>99 100 101 102 103 104 105 106 107 108 109 110...</td>\n",
       "      <td>(Evidence, 570.0, 1124.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117056</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621712e+12</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>It is a common belief that the best education ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>198 199 200 201 202 203 204 205 206 207 208 20...</td>\n",
       "      <td>(Claim, 1125.0, 1277.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117057</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621712e+12</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>2076.0</td>\n",
       "      <td>This may be true for many people but it is def...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>224 225 226 227 228 229 230 231 232 233 234 23...</td>\n",
       "      <td>(Evidence, 1277.0, 2076.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117058</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621712e+12</td>\n",
       "      <td>2077.0</td>\n",
       "      <td>2331.0</td>\n",
       "      <td>One of the biggest reasons illness can spread ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 2</td>\n",
       "      <td>359 360 361 362 363 364 365 366 367 368 369 37...</td>\n",
       "      <td>(Claim, 2077.0, 2331.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117059</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621712e+12</td>\n",
       "      <td>2332.0</td>\n",
       "      <td>3311.0</td>\n",
       "      <td>While they may be able to make it through all ...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 3</td>\n",
       "      <td>402 403 404 405 406 407 408 409 410 411 412 41...</td>\n",
       "      <td>(Evidence, 2332.0, 3311.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117060</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621864e+12</td>\n",
       "      <td>3312.0</td>\n",
       "      <td>3538.0</td>\n",
       "      <td>Many who oppose allowing distance learning may...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 3</td>\n",
       "      <td>580 581 582 583 584 585 586 587 588 589 590 59...</td>\n",
       "      <td>(Claim, 3312.0, 3538.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117061</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621864e+12</td>\n",
       "      <td>3539.0</td>\n",
       "      <td>3879.0</td>\n",
       "      <td>Often times students in class who are distract...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 4</td>\n",
       "      <td>614 615 616 617 618 619 620 621 622 623 624 62...</td>\n",
       "      <td>(Evidence, 3539.0, 3879.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117062</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621864e+12</td>\n",
       "      <td>3880.0</td>\n",
       "      <td>4054.0</td>\n",
       "      <td>Another point people who oppose optional dista...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 4</td>\n",
       "      <td>668 669 670 671 672 673 674 675 676 677 678 67...</td>\n",
       "      <td>(Claim, 3880.0, 4054.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117063</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621864e+12</td>\n",
       "      <td>4055.0</td>\n",
       "      <td>4452.0</td>\n",
       "      <td>While in some cases this may be true, the peop...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 5</td>\n",
       "      <td>697 698 699 700 701 702 703 704 705 706 707 70...</td>\n",
       "      <td>(Evidence, 4055.0, 4452.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117064</th>\n",
       "      <td>7D95539DC1AE</td>\n",
       "      <td>1.621712e+12</td>\n",
       "      <td>4453.0</td>\n",
       "      <td>5495.0</td>\n",
       "      <td>Giving students the option to have distance le...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Concluding Statement 1</td>\n",
       "      <td>763 764 765 766 767 768 769 770 771 772 773 77...</td>\n",
       "      <td>(Concluding Statement, 4453.0, 5495.0)</td>\n",
       "      <td>At first thought, distance learning may seem l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  discourse_id  discourse_start  discourse_end  \\\n",
       "117053  7D95539DC1AE  1.621712e+12              0.0          380.0   \n",
       "117054  7D95539DC1AE  1.621712e+12            381.0          570.0   \n",
       "117055  7D95539DC1AE  1.621712e+12            570.0         1124.0   \n",
       "117056  7D95539DC1AE  1.621712e+12           1125.0         1277.0   \n",
       "117057  7D95539DC1AE  1.621712e+12           1277.0         2076.0   \n",
       "117058  7D95539DC1AE  1.621712e+12           2077.0         2331.0   \n",
       "117059  7D95539DC1AE  1.621712e+12           2332.0         3311.0   \n",
       "117060  7D95539DC1AE  1.621864e+12           3312.0         3538.0   \n",
       "117061  7D95539DC1AE  1.621864e+12           3539.0         3879.0   \n",
       "117062  7D95539DC1AE  1.621864e+12           3880.0         4054.0   \n",
       "117063  7D95539DC1AE  1.621864e+12           4055.0         4452.0   \n",
       "117064  7D95539DC1AE  1.621712e+12           4453.0         5495.0   \n",
       "\n",
       "                                           discourse_text  \\\n",
       "117053  At first thought, distance learning may seem l...   \n",
       "117054  By allowing students the option to choose thei...   \n",
       "117055  In addition, giving students the option to dec...   \n",
       "117056  It is a common belief that the best education ...   \n",
       "117057  This may be true for many people but it is def...   \n",
       "117058  One of the biggest reasons illness can spread ...   \n",
       "117059  While they may be able to make it through all ...   \n",
       "117060  Many who oppose allowing distance learning may...   \n",
       "117061  Often times students in class who are distract...   \n",
       "117062  Another point people who oppose optional dista...   \n",
       "117063  While in some cases this may be true, the peop...   \n",
       "117064  Giving students the option to have distance le...   \n",
       "\n",
       "              discourse_type      discourse_type_num  \\\n",
       "117053                  Lead                  Lead 1   \n",
       "117054              Position              Position 1   \n",
       "117055              Evidence              Evidence 1   \n",
       "117056                 Claim                 Claim 1   \n",
       "117057              Evidence              Evidence 2   \n",
       "117058                 Claim                 Claim 2   \n",
       "117059              Evidence              Evidence 3   \n",
       "117060                 Claim                 Claim 3   \n",
       "117061              Evidence              Evidence 4   \n",
       "117062                 Claim                 Claim 4   \n",
       "117063              Evidence              Evidence 5   \n",
       "117064  Concluding Statement  Concluding Statement 1   \n",
       "\n",
       "                                         predictionstring  \\\n",
       "117053  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...   \n",
       "117054  68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 8...   \n",
       "117055  99 100 101 102 103 104 105 106 107 108 109 110...   \n",
       "117056  198 199 200 201 202 203 204 205 206 207 208 20...   \n",
       "117057  224 225 226 227 228 229 230 231 232 233 234 23...   \n",
       "117058  359 360 361 362 363 364 365 366 367 368 369 37...   \n",
       "117059  402 403 404 405 406 407 408 409 410 411 412 41...   \n",
       "117060  580 581 582 583 584 585 586 587 588 589 590 59...   \n",
       "117061  614 615 616 617 618 619 620 621 622 623 624 62...   \n",
       "117062  668 669 670 671 672 673 674 675 676 677 678 67...   \n",
       "117063  697 698 699 700 701 702 703 704 705 706 707 70...   \n",
       "117064  763 764 765 766 767 768 769 770 771 772 773 77...   \n",
       "\n",
       "                                     idx_label  \\\n",
       "117053                      (Lead, 0.0, 380.0)   \n",
       "117054                (Position, 381.0, 570.0)   \n",
       "117055               (Evidence, 570.0, 1124.0)   \n",
       "117056                 (Claim, 1125.0, 1277.0)   \n",
       "117057              (Evidence, 1277.0, 2076.0)   \n",
       "117058                 (Claim, 2077.0, 2331.0)   \n",
       "117059              (Evidence, 2332.0, 3311.0)   \n",
       "117060                 (Claim, 3312.0, 3538.0)   \n",
       "117061              (Evidence, 3539.0, 3879.0)   \n",
       "117062                 (Claim, 3880.0, 4054.0)   \n",
       "117063              (Evidence, 4055.0, 4452.0)   \n",
       "117064  (Concluding Statement, 4453.0, 5495.0)   \n",
       "\n",
       "                                                 raw_text  \n",
       "117053  At first thought, distance learning may seem l...  \n",
       "117054  At first thought, distance learning may seem l...  \n",
       "117055  At first thought, distance learning may seem l...  \n",
       "117056  At first thought, distance learning may seem l...  \n",
       "117057  At first thought, distance learning may seem l...  \n",
       "117058  At first thought, distance learning may seem l...  \n",
       "117059  At first thought, distance learning may seem l...  \n",
       "117060  At first thought, distance learning may seem l...  \n",
       "117061  At first thought, distance learning may seem l...  \n",
       "117062  At first thought, distance learning may seem l...  \n",
       "117063  At first thought, distance learning may seem l...  \n",
       "117064  At first thought, distance learning may seem l...  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['id'] =='7D95539DC1AE' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "88bceef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At first thought, distance learning may seem like an easy way out for kids who just want to skip school to not be held accountable for it; however, while it may seem counter-intuitive to allow students to decide to take their classes online, it can often times be very beneficial for both those who take advantage of the option and those who continue a traditional path of school '"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['id'] == '7D95539DC1AE'].iloc[0]['discourse_text']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be209a",
   "metadata": {},
   "source": [
    "# start tokenizing the entire datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0bb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a39343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6de566ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d75e784508420f839c367ce5794b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14034 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39b895436ae4b06bac61e3ca69afc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1560 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_assign_labels,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3f447159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "58bd1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c7233bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForTokenClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('allenai/longformer-base-4096', num_labels=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6c81fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./feedback1.2',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to='wandb',\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cd4f959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "162ee592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuh014\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b28101b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/yuh014/feedback1.2/runs/1kh6lc64\" target=\"_blank\">deep-snowball-1</a></strong> to <a href=\"https://wandb.ai/yuh014/feedback1.2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/yuh014/feedback1.2/runs/1kh6lc64?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa51d4acd30>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"feedback1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "389577c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: offset_mapping.\n",
      "***** Running training *****\n",
      "  Num examples = 14034\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2634\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  47/2634 22:26:37 < 1290:15:29, 0.00 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 746 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 924 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 881 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 976 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 958 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 937 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 815 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1023 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 804 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 828 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1005 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m0/r4gc8d8x2dz_h2ymv3jgxbv80000gn/T/ipykernel_39840/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m                 if (\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.8/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hook_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b24000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bf4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5f55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09ea5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02817f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef2f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3760ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac2d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab453b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bae9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
